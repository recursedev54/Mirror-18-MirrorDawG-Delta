so I'm Dave Roland I'm the CTO of the audio Squadron group we have two main size to our business on the prism sound side we have we make high quality audio interfaces and converters for large Studios and then on the traction side we do a lot of software we make some really in in-depth detailed synthesizers and then a daww or door waveform and the traction engine which is open source which it runs on so first up quick riddle when there's a door not a door when it's a jar is that twoo meta make more sense now okay right enough of the fun serious stuff from now on um so I'm going to start with a quote from Lex strumal who's the CEO of bronze which is a kind of an upcoming AI machine learning power door he States I can see why no one in their right mind would undertake building at door um and that kind of sets the tone for what we're going to talk about today this actually comes from an article that was doing the rounds a few months ago um from DJ mag called what is the future of the door can I just have a quick show of hands who's read this good yeah quite a few people so we we'll be drawing inspiration from this over over the duration of the talk soex goes on to say if you add the fact that people can't really make any money out of it because it takes so long to develop you can see why there's no innovation I think that that's actually a big part of it the economic incentives aren't there and that's really what we're going to talk about today is why does it take so long and therefore why are the economics incentives not there so I read through this article and tried to pull out some of the key terms that were kind of prevalent to building doors and I came up with this work cloud and you can see at the core here we have stability independ ability doors are mission critical pieces of software people rely on them professionals rely on them to get their jobs done and kind of strengthening that we have terms like Sacred Space muscle memory sacor sank keyboard shortcuts people need to use these tools without thinking about them and just know that they're in the zone being productive then we have Engineers producers and artists all very different types of users but still using the same tool so perhaps it's not tailored for each of those very different tasks and um professionals then we have the more negative terms like roadblocks exasperated frustrated no innovation you know these are the things that people don't like about doors and perhaps feel they could be done better there's another quote in this article from our very own Joshua Hodge from the audio programmer um and he States I feel like it's hit a plateau in terms of creativity and where a lot of people are just creating the same thing A lot of people are wondering what is the next thing what is it going to look like and how can we expand on the current capability of what we're doing with a door but I don't think anyone has quite nailed what it will look like and that is quite sort of prevalent it echoes the fact that we're perhaps at this Tipping Point where there needs to be a bit of a paradigm shift so I'm going to go back to the beginning of doors and the the tasks that they do and they're effectively created to replace um analog Studios so all the tasks that you're do in an analog Studio have kind of div digital equivalent do recording Arrangement playback exporting and bouncing file management used to replace all those analog tapes and pretty soon afterwards plugins came along and they were used to augment features of the DW or door um to provide functionality like synths effects and midi generators and the doors liked this kind of plug-in model because they had a very thin API and that to some extent sandbox what they can and can't do so um you often doors with their internal plugins that are really part of the door but presented as plugins uh for their own syns and effects and then plugins over time wanted to add more functionality so they wanted to get more of what the door has so you have things like Protools audio suite for reading and writing audio files melodine AR for kind of intercepting audio clips and regions for the door and then a whole bunch of extensions from Reaper personas amongst others that enable you to get at specific pieces information within the door so over time really the two have come together and when you want to build something that is both a plugin in a door you end up with an app and an app that does all of these things is really just a door okay so I'm going to reiterate that quote from Lex that we heard if you add the fact that people can't really make any money out of it because it takes so long to develop you can see why there's no innovation that's that's actually a big part of it the economic incentives aren't there so we're going to nail down on the economic incentives here um this next part is going to be quite quick but there's a lot to get through and hopefully it imparts the complexity that we're talking about so you can imagine uh product designers get together and then create a bunch of user stories in the agile world so you get a bunch of features um things like uh sorry user stories like I want to be able to arrange multiple audio files on a timeline and if you summarize that in this list that effectively becomes Arrangement so your product is a whole bunch of user stories that basically cover all of these features that doors do so we're going to try and quantify the costs of building something that is effectively a door so to do this I went to glass store and looked at uh the average salary for a senior software engineer in the audio industry this came out to nearly 73k which averages out about Â£300 a day or $375 and a quick note when I do these estimates um these include doing things properly right writing tests and validation because this is Mission critical software it needs to be stable needs to be Dependable we want to do things properly and we're going to have a running totalizer in the top right to see see how much doing things is going to cost us okay first the simple stuff um we need a notion of time right digital audio systems are Ty and the natural time in digital is samples that's not particularly useful um in the real world we tend to talk about things in terms of time in in seconds but we have a nice sample rate that can convert between those two and a simple equation that can do that for us there's a more musical way of talking about time though and that's in terms of Beats and we use the tempo to convert between those to we have a nice simple equation to do that and it can be rearranged as well to find the Beats if you've got a Tempo in some time and there's kind of one higher level concept of musical time which is bars and beats and I'm kind of glossing over a few complexities with the denominator here but effectively your um your numerator of your time signature gives you the number of bars so how long is it going to take to do this well if you're really good have a good idea of these Concepts then a couple of days write some tests you got some classes and Away you go but it's not usually that simple because in most modern applications you have Dynamic Tempo and that's either because you have a way of programming in a dynamic Tempo with ramps and curves and change in time signatures or you're chasing to some external Source it could be midi clock it could be Ableton link that kind of thing but effectively um Tempo can change you need to support that now I'm not going to go into all the the technical details but the rough way you do this if you've got um ramps and curves is you can chop those up into small sections figure out what the current state of your time is so the bar number the number of Beats aaps the time and then use that state to do some linear equations on the end um this is what the code to construct that sequence from um a a high level sequence of tempo changes in our traction engine codebase looks like of course there's curves so we need some bezier calculations in there and then once we've got that we have some calculations that can do our conversions between the two so building that if you're good couple of weeks okay we spent about 5k but it's probably going to take a bit longer than that to be honest okay we got a notion of time let's do something proper audio file reading what the problems with audio file reading it's simple right well it's not like we're making a player here like Spotify or apple music or something we typically in a in a door have many files there could be hundreds along a track or many tracks and also long files imagine you're recording a podcast or an or Orchestra or something or a gig um several hours then we have the fact that the transport position particularly when editing can jump around we've got loops and and editing positions that will change and we need to work around all those clips uh clicks sorry then we have clips and regions within those uh within the timeline that can be looped and of course this is a realtime system so we need to do it in real time and then hopefully that means weight free although Fabian mentioned yesterday we can just use locks everywhere that's fine um the problem is we we're essentially doing file reads which is a system C so potentially blocking operation um so we're going to need some threads in there so digging down onto this the the typical approach is we have a background thread that will load the files into memory and then the audio thread will come along and read from that memory that's that's how we get the speed but we can't keep all the files in memory because if we have large files those multi power files they'll probably be page to dis at some point so even though it looks like you're doing a memory read you actually be doing dis read and we're back to square one so the typical approach um is to keep track of all the upcoming read positions and then load into memory just sections so everything's nice and hot in Cache and we can read it um you can use memory mapping to help this it kind of offloads some of the work to um the operating system but it it typically works best for non-compressed files where you have a Ono one map in between sample position and time um on the other side of thing things the audio thread needs to read from this memory which means we need some hopefully weight free kind of synchronization around that memory in order to get the data and me and Fabian spoke about this um in quite quite de quite a lot of detail on this very stage a few years ago and if you want even more detail then Tim or gave a three-hour session on this uh CPP now so watch the two-part session from me and Fab then Tim was that's a whole day gone already um so in summary to kind of work around all these problems we're probably looking at about six months worth of Dev if if you're if you're Quicky if you're quick at it um but again it's not that simple because as I mentioned we have clear for region loot points so here's a drum sample imagine it plays you know from start to finish that's not always the case because it might be positioned um with a different start and end on the timeline and an offset so you don't start playing from the beginning and of course it can be looped as well so you can play from the middle of the file towards the end of the file and then jump back to the middle of the file which means we have different read positions at the same time and if you look at the transport you can see this same file four different read positions as we're going along and that probably means we need multiple read buffers even though we're only playing back one file and therefore multiple memory touches if we're doing memory mapping as well so to figure out this looping concept probably another couple of weeks worth of Deb okay transport position jump in I mentioned this happens a lot what does this mean well effectively when you're playing back the playhead can randomly jump to a new position um if the user's clicking and editing then effectively we can just be jumping around randomly so imagine your um and therefore the read positions in the files obviously jump um this means that any file caching you're doing into memory needs to be quick because you want to avoid missing samples if you jump to the start of a bar and then you miss all your kick and stare transients things are just going to sound a bit weak and not what you expect so again there's a few more tricks we need to employ here perhaps decompressing compressed audio format so they're quick to seek oh and we also need to work around all those clicks because if you happen to be playing a sample that is you know a high number and then you jump to a region where there isn't a high number you're going to hear that as an audible click and of course that's more difficult because you could be playing back clip that doesn't exist when you move the transport so you need to remember everything that was playing so you can fade it um how how long to work around these finger in the air a month but honestly you probably spend your life doing these kind of things transport position looping you know we have Global Loops in this as well as clip Loops it's similar to the just random transport position jumpins except that we know when the jump is going to happen and where it's going to jump too so that gives us some more information um but it might be in the middle of a block with the user clicking you can tend to schedule that on a block boundary but with transport if you're reading a thousand samples and the loop end happens to be in the middle you're probably going to need to chunk that so you don't bleed into the next transient um or miss the transients at the start of the loop and of course to keep everything in Sp sync especially with external sources needs to be accurate at and of course when we make jumps we our Tempo or time signatur might have changed so any kind of state that's reading audio files need to be updated for that so another month or so if we're lucky okay reading files is still not that simple because you probably need to do some time stretching because audio files they might not be at the same tempo as I say transport position here because remember we have Dynamic tempos and tempos changing all over the place as the things play back so we're going to need to do this on the Fly not only can the global trans uh the global Tempo change you might have changing tempos in your file Um this can happen if people manually apply Dynamic walks for musical or nice effects but also if you're recording something live that wasn't to a metronome you can apply a Tempo map to it so it plays quantized back with your session so you've got these kind of two different tempos going on that you need to sync and of course that can be looped as well so you need to deal with those cases and of course time stretching needs to be applied quickly without latency so when things jump around you get those samples and not either a glitch or um missing transits and things like that so I said another six months to do all this time stretch and stuff and believe me this is difficult I spent about six months recently adding Dynamic time stretching on top of all our existing static time stretching stuff um so we had a bunch of things to go on already and it it took a long time so a quick Round Up of how many features we've got out of our big list here well we've got a notion of time and beats we can read some audio files great it's taken nearly a year to do and a budget of about 100K also there's no UI here right we haven't built an application of any sort we've just built some very small building blocks some Foundation there's a big list the features still left to do so let's not one off okay right playback what do we mean by playback well um we've discussed taking samples and converting them to some kind of time or beats but what we really need is some kind of audio graph that we can build and then hand that off to the for to the device to process um and I'm not going to talk about that in detail because I spent an hour talking about it a couple of years ago um when ADC was online so if you want to uh look at the finer details and the problems there um certainly watch that talk how long is this going to take well it took me about a year to do this um but I'd been thinking about it for some time beforehand and I had our existing uh graph processing framework so I knew uh the good points of that and the bad points that we wanted to replace and also it was during lockdown so there wasn't much else to do um so that took me about a year so that's another 100K worth of budget um what the problems involved though well it needs to be flexible these things are going to change over time you're going to add different types of playback elements uh new nodes so it needs to be able to support that from day one or you're going to come unstuck needs to scale your app might be small to start with only playback a few tracks or a few plugins but pretty soon the product people are going to say oh it'd be great if you could do a thousand tracks so it needs to scale well um you need to persist State that's one of the big things that I was um discussed in the talk I just showed which is when your graph changes you need to continue playback without any sort of glitches add into that latency buffers and things and it all gets quite complicated of course it needs to work in real time this is an audio system and to take advantage of Modern Hardware you probably want to be able to process it using multiple threads um and I kind of carried on that talk from traction graph into the the ADC talk I gave last year which was how we um optimize these systems and make them actually work really and I gave a version of that talk at CPP on C which is a bit faster paced but has a few nicer examples so um maybe watch that that one if you're going to uh watch one of the two so another of you we've got the time Beats audio files and now we've got a playback graph that we can kind of pass to the audio system to play back but we've still not got any proper notion of arrangement so what do I mean by Arrangement well effectively what we're talking about is a data model so far we've just got files and um an audio graph that can play them back but we need to object we need to have objects in that model and to be able to specify properties of them so our objects would be things like Clips or regions and the properties would be start length source file those kind of things so this is a much higher level concept than a playback graph um it usually Maps fairly well to UI if you kind of think of the the terms the properties like start length Etc um so you've got links to UI from your data model but also the audio graph needs to be able to build from this so it kind of needs to be able to read it you got two uh different kind of concerns looking at the model and finally the UI will actually usually be built on top of this well the some some of the problems with building a data model well at some point is probably going to need to be serialized you're going to store it either in the cloud or on diss so you've got to think about file formats perhaps versioning backwards compatibility from the off that's very hard to retrofit again scale as these things grow over time or people add MP files with millions of midi points of gestures and things you know how is that going to be handled needs to be expandable requirements change over time features get added so you need to have be able to support those undo redo is a pretty standard feature these days copy paste um pretty hard to use an app with that without these kind of things and also you need to think about how these are going to be shared around and that often relates file references with audio you know even an external drive how do you make sure all your resources are still there um and also to friends on different machines um and I talked about data models uh what six years ago now at ADC and the approach we took using juice value trees um in traction engine and how long does it take to do that well to do our conversion I'm saying six months here if you start from the beginning it took me Ang Jules about a year to convert traction engine but we had a bunch of features that you might not have if you're starting an app um so yeah six months if you're lucky let's put put that on the totalizer okay I mentioned the the data model needs to um or the the playback graph needs to be built from the data model and we do this in about 2,000 lines of code in traction engine but it's not always straightforward because the way it looks in UI in the way it's played back are often two very different things um some common problems are things like Orcs send and returns or buses um in the model they're just bus numbers and positions you know pre- or post effects but in the playback graph they might need to create multiple nodes perhaps with shared buffers um gains applied to send some things together and all the ordering and latency needs to be handled correctly another common problem track mute and solo so these are just two properties of a track um you know mute and solo but they control whether all of the tracks should be audible or not and what happens if you have things like upstream or Downstream tracks when you've got folders groups and submixes how does mute and solo interact with those and of course this needs to be highly optimized because it needs to actually work um imagine places where you have a track that's 2 hours long and you've got hundreds of Clips on there you don't want to process a node for each of those clips you want to process you only want to process the clips that are near the playhead so thinking about a way to do that appropriately can be complicated and when you mute or bypass plugins um or even tracks this can be tricky for some reasons I'll talk about a bit later on but due to latency you often want to apply your own latency to a plug-in as a clean signal so you can bypass it it effectively on and off without kind of dropouts or glitches so there are all these kind of gnarly edges you need to deal with so let's put some more time on the board for figuring out how to do this okay audio recording uh we're moving now this is kind of the flip side to audio playback um and therefore it's kind of inversed problems we the fundamental problem is that we're we have real-time audio buffers coming at us and we want to write these to disk without dropping any um and and we have a real-time audio thread that's going to end in a file right which is going to be a system call which is potentially blocking so we we need to kind of Marshall that system some way and Mis blocks in recorded audio are effectively impossible to recover you imagine you're recording a live concert that's going to go out on the BBC or something like that and suddenly you miss a load of blocks recording to disk you've effectively wasted that entire recording you'll never get it back um we also need to deal with a device latency because if there's latency on the inputs then when you align things on your timeline to perhaps already existing material everything needs to line up and then we have looped or take recordings that we need to do uh punch in and out make sure everything lines up and also couns usually as well and at some point this is probably going to turn into an app with the UI so we need to think about providing this incoming audio data so it can be thumbnailed so what are the typical approach Roes here well we uh can pre-allocate a bunch of audio buffers that kind of float around and then the audio thread can pick those up copy their data into it and then pass that off to a thread to do some writing so we're probably going to need some kind of Lock Free que um whether it's single producer single consumer or one of the multiv variants which can get pretty hairy pretty quickly is dependent on your system but you almost certainly have multiple inputs being recorded at once so you have multiple inputs perhaps fighting over those buffers um to pass them onto the writing thread that's going to flush them to disk some of the problems well what happens if you run out of pre-allocated blocks Um this can happen if your dis rights can't keep up with the audio input rate imagine you're recording 64 or 128 channels and then your uh desktop operating system decides to do some backup in the background that you didn't know about you know how do you handle those situations what about changing buffer sizes you've preal at a bunch of blocks at 128 and then someone says well this isn't quite working we'll go to 256 and suddenly none of your buff buffers fit when do you reallocate those and Supply those to the audio uh audio system um changing Channel numbers similar problem but you could have a monot TR you could have a stereo input you could have a surround input and what buffers do they use you use multiple cues that have different channels or do you pre-allocate everything to be 64 channels that's a big waste of memory perhaps apps things that you need to think about um failed file rights what happens if your disc is becoming full um this is pretty disastrous but you need to think about how you're going to relay that back to the user so they can act appropriately do you need to track disc progress perhaps preempt how long they're going to record for so you can um relay sensible workarounds um how long's that going to take again if you're lucky six months but you're doing well if you're doing six months okay on to MIDI so we're kind of done with the audio side of things for now but we've got midi um first of all playback now midi is event based we have things like no on or off or CC changes Etc it's not stream based like audio is which means we can't reuse anything from the audio pretty much there is one upside to this in the midi tends to be less data each midi packet is only a few bytes um but there's another disconnect in that in your model you usually think about things in terms of notes um but when you play them back they're converted to an event list and the difference is that notes have a start a length a pitch Velocity channel number Etc which is where we get the the piano roll from but then when we play them back we have just a bunch of onoff events so let's look at the things that can go wrong here uh most midi problems are kind of categorized into two the first of which is missing notes so imagine you have your transport playing from start to finish you passed all the note events great um but what happens if you start playing from the middle of a sequence from here you might think I can just skip it but what if it's a an 832 bar pads you might want that to come in straight away so you you need to kind of look into the sequence scroll back to find any notes that should be on and start them but also any CC messages because the program might have changed at this point so you want to make sure your synthesizers actually receive upto-date CC information so they're doing the right thing um and then you can kind of continue playing your sequence and this may sound like a bit of an edge case but it can actually happen in loads of situations um if you're undoing or redoing copy and paste or duplicating notes the transport is looping looping around if you do pitch changes applying grooves quantize swing all of those things can effectively mean the notes appear underneath the playhead the other side of things is stuck notes and you get stuck notes when a note is playing it's no on event has been sent that then either the playhead jumps or the note has been deleted and by deleted that could be its Channel numbers changed or its pictures changed as well as actually fully removed um then you need to figure out when you're going to send the note off for that a value so there's a few situations this can happen play head jumps see you're playing along and then suddenly uh the user clicks somewhere else and you're never going to reach that not off event you need to make sure it gets sent at some point um looping very similar to before um we never reach that note off so we need to make sure we send it before the loop loops around because otherwise we're going to get that note ringing out and it will interrupt any other notes of the same note number that appear in that sequence and the big one is is a deleted note so we're at this point playing back a note gets deleted and now that note off event just doesn't exist anymore it's not even in the sequence so we need to kind of keep an idea of all the notes that are playing and then track these events and then know which notes to turn off at the appropriate time and of course this is complicated in a door situation because you have multiple midi sources you could have live inputs being combined with playback from Clips you can have multiple clips you could have multiple tracks being some together to go into a synth you can have sends you can have midi generators like ARS or chords All generating notes and things so um you have to deal with this in all of these scenarios so that's easily going to take another six months worth of work okay mini recording on on the golden path this is actually pretty straightforward we have a mini thread which is usually separate to the audio thread and because midi notes are small we can allocate a large buffer and then write notes into those buffer and because the mid midi thread isn't your real-time audio thread it's less time sensitive there are caveats to this because you still need to get the notes to the audio thread so they can be played back without some huge latency but for recording purposes um you've got a bit more time what's some of the problems well what if you're quantizing incoming notes the note that comes in isn't the note that you actually want to pass on to the audio thread or record it's a modified version of it um what happens if you want to play back the sequence whilst you're recording ing it I live looping um the problem here is that we have a linear stream of events that are coming in effectively second time stamps and then we need to convert these to beats so they end up at the correct position when they're stamped onto the timeline again another three months if you're lucky midi sequencing this is similar to clip or region Arrangement um but what you're doing really is modifying a sequence that is playing back and therefore all of those stuck and missing notes just rear their head all over the place because you're changing the sequence as it's playing and of course you need to add features such as quantization Groove swing which is really just a special case of GrooVe and don't even get me started on MP because that's a whole other uh can of worms so to do some good sequencing you're probably looking about another six months let's do another review here we've got our time beats we can read and write audio files we've got our data model we can save and load our state we can undo redo we've got some hooks for UI but no UI yet remember this is not an application it's a it's an engine or the start of an engine we've got a playback graph that can read our model and be passed to the audio device and we can record sequence and Playback midi so we've got a few more things to knock off um so let's go quickfire here plug-in hosting at some point someone's going to say hey it' be really cool if we could host plugins um that's all right because Juice handles all of that for you right well kind of but it does the low end side of things we still have to think about when to initialize and de initialize plugins and how to slot them into our audio graph we have to think about deletion plugins often don't like being deleted um without having their message Loops flushed so we might need to do that um kind of asynchronously there is precisely one person in the world that understands Channel buses so if you have to deal with those good luck getting a hold of him um there's State loading and saving plugin of State you save it right right well do you want to be saving or flushing all of your plug-in States every time the user presses save or when you're autosave time goes off probably not uh that's unnecessary work so you need to track when things have actually changed and Automation and we'll talk about that in a bit more detail so to add some plug-in hosting if you're using juice perhaps another three months so one of the problems with plugins is they can introduce latency and therefore we need plug-in delay compensations um what effectively happens here is that plugins can lead to offsets in the times that subsequent nodes in your graph um are seeing so we need to compensate for these delays and I've got a a visual I can show in the next slide but we also need to synchronize the AUD audio that's being heard with audio that's coming in and being recorded so there's an issue there and also we need to ensure that plugins are reading the correct automation values one once they've been delayed um otherwise things will kind of drift out of syn and this is extra complicated um when we have Orcs buses so to get this kind of thing uh working I'll talk about bypassing in a sec another three months PDC is is hard and to get it right in all situations is particularly hard so imagine we've got this example we've got our drum loop again it's on a track and then we've got two other tracks we have it going through an orend which is effectively bu number one then that signal goes through a delay plug-in this is really a convolution Reverb or a compressor a limiter we look ahead something that introduces delay um in the signal and then we are we have a second track that is taking a return from the first track send going through another instance of a similar plug-in and then going out to a second bus and then a third track that is receiving that second bus and then going through another delay and then the first track is also receiving that second bus and being sumed together yeah this sounds contrived but I've seen these situations um from users and also we have automation here so if we build a simplified graph of what happens here imagine um the audio file reads A Time second of a time stamp of one the first two delay plugins actually start proing processing that Tim stamp but then when we go through ask uh sorry the signal is delayed effectively by quarter of a second here because it makes the maths easy um and then the second delay plugin delays another quarter of a second and we end up trying to rectify these 0.5 and 0.75 time stamp blocks so that's not going to work so what we need to do is introduce another block of latency so everything um balances out and we can safely sum those signals together but remember our friend automation here imagine our transport is playing at this position and this is the value that we send our automation updates to the plugins that's not going to work for this second delay plugin because oh sorry it's not going to work for the second delay plugin because it thinks that the audio it's playing is actually here so we need to make sure that our time stamps that we're um sending automation values for and also time stamps that we're sending to the plugins in terms of what the current number of Beats and bars is if they're doing any internal sequencing or beat sync um effects then they need to know that they're not processing what the rest of the graph is processing and actually this is a simplified case because Angus brought up in his talk yesterday the fact that you can be looping and this could be the start of your Loop so the fact your second delay plugin here could actually be processing your Loop end so the automation value it's reading isn't really this minus 0.25 it's actually like plus a few seconds or something like that okay at some point once we've done all of this we're going to want to render it out to um probably a stereo audio file or some midi something like that but that's a whole headache in itself we need to detach it from the audio device um we can't be playing something back whilst rendering it that's not going to work we need to make sure we flush and reset all of our plugins we don't want things like delay or Reverb Tails suddenly appearing at the our our exports at the start of our exports we need to think about what tracks are we going to include yes you're doing a um like a a main bounce down then all of them but what if you're doing a freeze or a render something like that um do you include plugin side chain inputs do you include the outputs of those tracks what if you have groups folders submix do you include the destinations in them what if you've got orcses do you include the um the sent trxs do you include the output of those sent TRS when you're doing these renders and once you figured out all of those decisions you've actually got to do the rendering you have to deal with um audio formats and midi which are very different you might need to think about different sample rates different bit depths normalization whether you're doing it with Peak or RMS plug-in Tails I this sounds silly but you probably want plug-in Tails if you're doing a render you know bouncing track to track or a freeze but when you're doing an export for a loop you probably don't want those because you want it to loop around nicely do you trim silence a features often useful for doing podcasts and things like that what about dithering what if how is this going to be resampled in the future and of course if anyone plugs in some outboard gear then you need to think about doing things in real time because um faster than real time outboard gear doesn't usually work so if you're lucky you could get this done in three months let's do another quick review then we've kind of got most of the features now we've got all of our notion of Beats times playback and Etc but we can host plugins and we can explore audio how long has it taken well that's nearly six years and nearly half a million pounds so quite long if we look at our high level feature set the things based on our user stories there isn't a direct onetoone mapping because some of these features are required for multiples of them but I've kind of broken it down here so you can see the rough weightings that's easier to see if we look at it on a pie chart but the problem here is that all of these are pretty essential apart from maybe you don't want midi you you certainly want some playback you need to do your file management at some point you're export it there's going to be some element of mixing sequencing editing Arrangement you know all of these are fundamental features this is the start of an application this isn't the end and of course there's a whole bunch of stuff missing from this we've only talked about the engineering tasks here we haven't talked about any kind of product design any UI development we haven't talked about any testing or QA that gets done through the process towards the end of this kind of engineering phase you probably need to build a website use a manuals and all the stuff that goes into that and also customer support you're going to need to train people on what you're building so they can inform other people so and again I feel like I've been pretty generous here you could easily treble what we've already got to build a product and now we're looking at nearly 20 years worth of single development and nearly one and a half million pounds now of course it wouldn't be one person doing all these you can throw more people at it but if you think just in terms of engineering if you had five Engineers to this they're not it it doesn't scale lineally they're not going to do the job in 20% of the time um you need to think about how those communicate what tools they use asynchronous working um managers to oversee them perhaps you've got more Junior people that you need time to support and train so yeah it's still going to take a long time the other thing that's missing and it wouldn't be an ad23 talk if I didn't talk about AI is is some kind of artificial intelligence and I want to talk about artificial intelligence as a tool not a feature um there are a bunch of features that where AI is a particularly good tool things like stem separation vocal or instrument swapping um intelligent mix effects like EQ compression compression mastery and often mixing needs access to multiple tracks so it can intelligently um perform a good job and then we have our generative AI such as midi drums vocals and even complete pieces of music from text and these tools or features kind of sit in two buckets you have things that are complimentary to door workflows and then the kind of one button solutions that you just music from text say make me a a banging house track and off it those but for most use cases or i' say most professional use cases you probably need all this engine stuff that I've talked about to put your AI tools on top of right it probably needs access to your audio files or your audio streams at least to do something you might need to render down what you're doing in order to pass to the AI based tools so it can then send something back to you and you need to reinsert it back into your model the the engine is the starting point here of a product and Declan who's the author of The Mix mag uh the DJ mag article um kind of summarizes this nicely he says artists want to be creative and they want to have control over the creative process for Tik toks or podcast music soundtracks this throwaway generative music might work but for artists and producers with their own unique identity it's hard to see the one button style solution working what's more likely is that generative AI replaces steps within the music making process rather than replacing creativity entirely and I wholeheartedly agree with this now if I think about things that I could have called this talk I was I struggled a bit with what to call it I could have called it how to Crowbar in as many references of my past talks as possible which sounds like a joke but there's actually a serious Point here in that I've been building doors for about a decade now and every year I've come across a signif L complex problem or chunk of work that has given me enough to talk about in an ADC talk that other people found interesting hopefully um so you know this this is this is important I could have called it something very vanilla like behind the scenes of a door engine because effectively I've just listed all the things that we do um and if you work on a door perhaps you could call it this how to petition your boss for a pay rise this is hard stuff and show them this and they might go well you're doing a good job but let's go back to what I landed on which was why you shouldn't write a door and perhaps that was a bit misleading I probably should have called it why you shouldn't write a door engine and that's because perhaps there's another approach you can use traction engine and yes this is plug time um I'd like to see other approaches to this but this is why we've open sourced the engine and again if I can get one more talk in there me and Joel spoke about this a few years ago quite a few years ago nowhere um at the London audio developers Meetup so let's just look at those costs because we've been talking about costs a lot today imagine you're a company with less than $2 million Revenue which is you know standard seed money or early early rounds of funding um our costs are $150 per seat per month imagine you have a team of five devs which is not unusual you're looking if we take those salaries from before nearly over three and a half ,000 in salaries or you could pay $9,000 in traction engine license fees which is only 2 and a half% of your wage bill when you think about it like that that's a pretty good deal and also you get that day one so you don't have to spend those years of of engineering on it um and also we've spent a long time solving all these problems that I've spoken about and we haven't stopped we're still building out features that you can you can get one feature that we're adding to waveform and therefore the traction engine very soon is clip launching and here's a quick example of what it currently looks like we're just kind of building out the fishing touches to it so you could use that um other options well you could partner with us we're always open to people with good ideas um come and talk to us me jores Woody are all here at the conference you know wouldd love to continue our journey with with the right people so I'm going to go back to the quote now from Lex that I opened with and I'll just reiterate it if you add the fact that people can't really make any money out of it because it takes so long to develop you can see why there's no innovation I think that's actually a big part of it the economic incentives aren't there but what if you could take away the fact that it takes a long time then you could take away the fact that the economic incentives aren't there and hopefully what you're left with then is just Innovation and I think that's why you shouldn't write a door engine to write something new something Innovative thanks very much [Applause]
